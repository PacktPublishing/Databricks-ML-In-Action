# Chapter 4: Exploring and Cleaning Toward a Silver Layer

**Here is what you will learn as part of this chapter:**

1. Exploring data with the Databricks Assistant
2. Generating data profiles with AutoML
3. Visualizing data with DBSQL
4. Setting data quality expectations with DLT
5. Applying our learning

## Technical requirements 

Here are the technical requirements needed to complete the hands-on examples in this chapter:
- The [Databricks Assistant](https://docs.databricks.com/en/notebooks/notebook-assistant-faq.html) is a newer feature that an administrator can enable. We will show the Assistant in this chapter.
- We use the [missingno](https://pypi.org/project/missingno/) library to address missing numbers in our project data. 
- In the section using AutoML, we reference the [AutoML-generated notebook](https://github.com/PacktPublishing/Databricks-Lakehouse-ML-In-Action/blob/0dbe21cdd3e11ff9295048d4d07bec14d037150e/Chapter_4_Cleaning_and_exploring/Favorita%20Forecasting%20Exploration/Autogenerated%20Data%20Exploration%20Notebook.py), which you can find in the GitHub repository. 
  
## Links

**In the chapter**
- [ydata-profiling](https://ydata-profiling.ydata.ai/docs/master/index.html)

**Further Reading**
- [Enabling visualizations with Aggregations in DBSQL](https://docs.databricks.com/sql/user/visualizations/index.html#enable-aggregation-in-a-visualization)
- [Using the ydata profiler to explore data](https://ydata-profiling.ydata.ai/docs/master/index.html)
- [Advancing Spark - Meet the new Databricks Assistant](https://youtu.be/Tv8D72oI0xM)
- [Introducing the new Databricks Assistant](https://www.databricks.com/blog/introducing-databricks-assistant)
